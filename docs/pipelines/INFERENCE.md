# Nexa Inference Engine

> **Scope**: Model Serving and Tool Control.
> **Modules**: `nexa_inference`

The Inference Engine is responsible for deploying trained models and managing agentic tool interactions. It provides a standardized API for model inference and stateful tool execution.

## Core Components

### 1. Inference Server (`nexa_inference/server.py`)
A high-throughput serving layer compatible with OpenAI's API specification.
*   **vLLM Integration**: Uses vLLM for state-of-the-art inference throughput on GPUs.
*   **Endpoints**: `/v1/completions`, `/v1/chat/completions`.

### 2. Tool Controller (`nexa_inference/controller.py`)
Manages the execution of tool calls generated by the model.
*   **Parsing**: Extracts structured tool calls (JSON) from model output.
*   **Execution**: Routes calls to the `nexa_tools` service.
*   **Feedback**: Returns tool outputs to the model for the next turn.

## Usage

```bash
# Start the inference server
python -m nexa_inference.cli start --model-path artifacts/checkpoints/latest --port 8000
```

