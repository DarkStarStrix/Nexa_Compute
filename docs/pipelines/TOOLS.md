# Nexa Tool Server

> **Scope**: Sandboxed Execution, External APIs, and Agentic Utilities.
> **Modules**: `nexa_tools`

The **Tool Server** is a dedicated microservice that provides "hands" to the AI models. It exposes a set of safe, deterministic capabilities via a REST API, allowing the Inference Engine's `ToolController` to offload complex tasks like calculation, coding, and research.

## Architecture

The server is built on **FastAPI** and runs as a sidecar to the inference service. It is designed to be stateless and secure.

### Server (`nexa_tools/server.py`)
*   **Port**: Defaults to 8000 (or configured via CLI).
*   **Endpoints**:
    *   `POST /python/run`: Execute Python code.
    *   `POST /papers/search`: Query academic databases.
    *   `POST /papers/fetch`: Retrieve paper metadata.
    *   `POST /units/convert`: Physical unit conversion.
    *   `POST /think`: Allocation of "scratchpad" computation.

## Core Tools

### 1. Python Sandbox (`nexa_tools/sandbox.py`)

A secure environment for executing model-generated Python code.

*   **Isolation**: Code runs in a temporary directory (`tempfile.TemporaryDirectory`).
*   **Timeout**: Enforced wall-clock timeout (default 10s) to prevent infinite loops.
*   **Artifacts**: Automatically detects and "harvests" files generated by the script (e.g., PNG plots, CSVs) saved to the working directory.
*   **Return Value**: Captures `stdout` and `stderr` separately.

**Security Note**: This sandbox is currently process-based. For production, it should be nested inside a container or gVisor sandbox to prevent host system access.

### 2. Paper Search & Fetch (`nexa_tools/papers.py`)

Provides access to scientific literature, primarily via the **Crossref API**.

*   **Searcher**:
    *   Accepts natural language queries.
    *   Returns a list of papers with Title, Authors, Year, DOI, and Abstract (if available).
*   **Fetcher**:
    *   Takes a DOI (Digital Object Identifier).
    *   Returns canonical metadata including BibTeX citations and full URLs.
    *   Used to verify hallucinationsâ€”if a model cites a paper, the system fetches the DOI to ensure it exists.

### 3. Unit Converter (`nexa_tools/units.py`)

A deterministic calculator for physical units, backed by the **Pint** library.

*   **Purpose**: LLMs often struggle with precise unit math (e.g., converting "molar" to "millimolar" while adjusting volume).
*   **Functionality**: Parses string expressions (e.g., "10 mg/mL") and converts them to target units, ensuring dimensional consistency.

### 4. Think Tool

A specialized tool that allows the model to "pause" and generate internal reasoning before emitting a final answer or tool call. It effectively buys compute time for chain-of-thought processing.

## Usage

**Starting the Server:**
```bash
# Usually handled by the orchestration script
uvicorn nexa_tools.server:app --port 8000
```

**Client Usage (Python):**
```python
import requests

# Execute Python code
response = requests.post("http://localhost:8000/python/run", json={
    "code": "import math; print(math.pi)",
    "timeout_s": 5
})
print(response.json())
# {'stdout': '3.141592653589793\n', 'stderr': '', 'artifacts': []}

# Search Papers
response = requests.post("http://localhost:8000/papers/search", json={
    "query": "attention is all you need",
    "top_k": 1
})
```

## Integration with Inference

The `ToolController` in `nexa_inference` automatically routes JSON-formatted tool calls from the model to this server.

1.  Model emits: `~~~toolcall {"tool": "python.run", "args": {"code": "..."}}~~~`
2.  Controller parses JSON and POSTs to `/python/run`.
3.  Server executes and returns result.
4.  Controller formats result as `~~~toolresult ...~~~` and feeds it back to the model.
