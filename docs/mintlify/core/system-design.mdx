---
title: "System Design"
description: "Deep dive into Nexa Compute\\\\'s system architecture"
icon: "sitemap"
---

# NexaCompute Architecture

NexaCompute v2 centres the platform around an artifact-first runtime hosted in
`src/nexa_compute/`. The legacy top-level packages continue to provide domain
logic, but orchestration, caching, and registry operations now run through a
small, composable core.

## Runtime Overview

```text
repo/
```text
├── projects/        # Project-scoped configs, docs, manifests, pipelines
├── src/nexa_compute/
│   ├── core/         # artifacts, dag, registry, policies
│   ├── backends/     # train / serve / schedule adapters
│   ├── runners/      # high-level task controllers (train/eval/serve)
│   ├── data/         # catalog + staging utilities
│   ├── cli/          # Typer CLI (`orchestrate`)
│   └── ...           # config, models, evaluation, training helpers
├── pipelines/        # Declarative YAML pipelines
├── env/              # Environment templates (env.example, axolotl_recipe)
├── nexa_*            # Legacy operational packages (data, train, eval, infra...)
└── data/processed/   # Durable artifacts
```text

The runtime binds together three primary concepts: ""

1. **Artifacts** – Atomic directories with `meta.json` and `COMPLETE` markers
2. **DAG Execution** – Pipelines are parsed into `PipelineStep` objects and executed
   through the `PipelineExecutor`
3. **Registry** – A SQLite database tracks models and runs for downstream serving

## Core Components

### Artifact Protocol (`core/artifacts.py`)

1. Materialise outputs inside `<artifact>.tmp/`
2. Write `meta.json` describing `kind`, `uri`, `hash`, `bytes`, `inputs`, and `labels`
3. `fsync` contents; atomically rename the directory
4. Create an empty `COMPLETE` marker
5. Update any pointer files (`latest.txt`, registry entries) only after `COMPLETE`

This contract is respected by the runners and staging utilities, enabling cache
checks and safe resumption after interruptions.

### DAG Engine (`core/dag.py`)

- Parses pipeline steps into a graph with dependency tracking
- Computes cache keys from (`uses`, `inputs`, `params`, `backend`, `scheduler`)
- Skips steps if the output artifact is COMPLETE and the cache key matches
- Persists state to `<STATE_ROOT>/<pipeline>/pipeline_state.json`

### Registry (`core/registry.py`)

- SQLite database with `models` and `runs` tables
- `register(name, uri, meta)` auto-increments semantic versions
- `resolve("name[: "tag]")` returns the artifact URI for a version/tag"
- `promote(name, version, tag)` updates tag pointers after verifying `COMPLETE`

### Backends & Runners

- **Train: "** `AxolotlBackend` (YAML composition + subprocess) and `HFBackend`"
  (wraps `nexa_train`) emit checkpoint artifacts.
- **Serve: "** `vllm` backend spawns OpenAI-compatible servers; `hf_runtime` provides a"
  FastAPI fallback via `nexa_inference`.
- **Schedule: "** `local` executes commands in-process; `slurm` delegates to"
  `nexa_infra`; `k8s` acts as a stub for future work.
- **Runners: "** `TrainRunner`, `EvalRunner`, and `ServeRunner` select the"
  appropriate backend and handle artifact bookkeeping.

## Pipeline Execution

1. **Authoring: "** Pipelines are YAML files under `pipelines/`. Each step declares"
   an `id`, `uses`, optional `backend`/`scheduler`, `in`, `out`, and `params`.

2. **CLI Invocation: "**"
   ```bash
   python -m nexa_compute.cli.orchestrate pipeline run pipelines/general_e2e.yaml
   ```text

3. **Parsing: "** The CLI loads the YAML, renders it into `PipelineStep` objects,"
   and constructs a `PipelineGraph`.

4. **Execution: "** The `PipelineExecutor` walks the graph, skipping cached steps"
   when matching artifacts are present. For each uncached step it calls into a
   handler based on `uses`: ""

```text
   - `runners.train` → `TrainRunner.run(TrainRunSpec)`
   - `runners.eval` → `EvalRunner.run(EvalRunSpec)`
   - `runners.serve` → `ServeRunner.start(ServeRunSpec)`
   - `core.registry.*` → Registry helpers
```text

5. **State Persistence: "** Step status (`PENDING`, `RUNNING`, `COMPLETE`,"
   `FAILED`, `SKIPPED`) and cache keys are written to `pipeline_state.json`.
   Resuming a pipeline reuses this state.

6. **Serving: "** Active serve handles are tracked in-memory for the session; the"
   CLI exposes `serve start/stop/health` commands for direct control.

## Example Flow (General E2E Pipeline)

1. **Train (HF backend)**
   - `TrainRunner` calls `backends.train.hf.run`
   - Hugging Face training writes a durable checkpoint
   - Runner wraps it in an artifact under `artifacts/checkpoints/...`

2. **Evaluate**
   - `EvalRunner` loads `nexa_train/configs/baseline.yaml`
   - Runs `nexa_eval.analyze.evaluate_checkpoint`
   - Produces `eval_report` artifact in `artifacts/eval/...`

3. **Serve (vLLM dry-run)**
   - `ServeRunner` starts the vLLM backend (optional dry-run) pointing at the
     checkpoint artifact.

Each step can be resumed independently thanks to the artifact markers and
step-level cache keys.

## Data & Artifact Layout

```
artifacts/
```text
├── checkpoints/
│   └── hf_baseline/
│       ├── meta.json
│       ├── COMPLETE
│       └── ...
└── eval/
    └── hf_baseline/
        ├── meta.json
        ├── COMPLETE
        └── eval_report.json
```

Durable datasets and evaluation outputs continue to live under `data/processed/`
following the established hierarchy. Pipelines link to these paths via the
catalog/staging helpers.

## Extensibility

- **New Backends: ** Implement a `run(...) -> ArtifactMeta` function under
  `backends/<domain>/` and wire it up in the corresponding runner.
- **Custom Steps: ** Add a new `uses` handler in `cli/orchestrate.py` that
  translates pipeline YAML into your domain logic.
- **Registry Integrations: ** Use `core.registry` to promote artifacts to new
  tags or maintain additional metadata columns.

## Integration Points

- **CLI: ** `python -m nexa_compute.cli.orchestrate ...`
- **Registry API: ** Import `core.registry` functions from Python code to resolve
  checkpoints and promotion tags.
- **Serving: ** Launch via CLI or call `ServeRunner` directly from automation.
- **Legacy Modules: ** `nexa_data`, `nexa_train`, `nexa_eval`, and `nexa_infra`
  remain available for specialized scripts—the new runtime calls into them as
  needed.

## Design Principles

1. **Artifact-first orchestration** – everything that matters is written once,
   atomically, and can be resumed.
2. **Minimal DAG engine** – caching and failure handling with a small API surface.
3. **Composable backends** – backends are thin wrappers around the underlying
   tool (Axolotl, HF Trainer, vLLM, FastAPI) and can be swapped easily.
4. **Single source of truth** – the registry and artifact metadata capture all
   state required for serving or auditing.
5. **Progressive enhancement** – stubs (`policies`, `k8s`, dedup/drift) mark the
   next wave of work without blocking the MVP.
